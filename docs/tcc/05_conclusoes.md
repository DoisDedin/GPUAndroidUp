5 Considerações finais

5.1 Síntese dos resultados

Este trabalho avaliou pipelines de Mean Absolute Deviation (MAD) e Transformada Rápida de Fourier (FFT) em dispositivos Android reais, confrontando implementações em CPU Kotlin com delegates do TensorFlow Lite (CPU, GPU e NNAPI) sob diferentes escalas de dados (512 → 65 536 amostras) e configurações de lote (single e x10). A suíte oficial executou cada botão cinco vezes consecutivas com 12 iterações/lotes por cenário, produzindo 640 medições por dispositivo (e mais 176 linhas extras no Moto G84 devido às tentativas experimentais acima de 64 k, que falharam por falta de memória).

Para o MAD, os speedups variaram de aproximadamente 4× em dispositivos com CPUs mais potentes (Galaxy S21 e Moto G84 5G) até mais de 100× no Moto G04s quando se chega a 65 536 pontos — a CPU levou 3,89 s enquanto o delegate GPU concluiu em 33,5 ms. Em todos os aparelhos, o batching x10 reduziu o tempo efetivo por pacote e elevou o throughput, confirmando a importância de amortizar o custo fixo de transferência ao projetar pipelines embarcados. Já na FFT os ganhos foram modestos em aparelhos topo/intermediário (1,5× a 2×) e ainda relevantes no Moto G04s (≈10×), mas vale destacar que todos os delegates fazem fallback para XNNPACK porque o TensorFlow Lite não possui kernel RFFT específico para GPU/NNAPI. Assim, apesar de carregarem o mesmo `.tflite`, as execuções ocorrem integralmente no CPU — e a diferença entre delegates reflete o custo de cópia entre buffers e os ajustes de precisão (FP32/FP16) de cada backend. A análise detalhada evidenciou que 70–85 % do tempo gasto pelos delegates está associado à transferência de dados, fator que limita a vantagem em cenários FFT e torna o delegate CPU quase tão eficiente quanto GPU ou NNAPI quando o gargalo se mantém inalterado. Ainda assim, descarregar o processamento para o interpreter TFLite reduziu o tempo ativo dos núcleos e mitigou picos térmicos nas três plataformas avaliadas.

5.2 Limitações do estudo

Os resultados aqui apresentados refletem apenas três smartphones, o que restringe a generalização para outros perfis de hardware ou versões de sistema operacional. O escopo focou em duas operações de referência (MAD e FFT) e não abrangeu redes neurais completas ou modelos híbridos, ainda que essas operações apareçam em diversas arquiteturas de aprendizado. Além disso, os sinais utilizados foram gerados por um processo sintético baseado em acelerômetro; embora representem padrões realistas, não substituem a variabilidade de dados coletados em campo. Por fim, as métricas energéticas derivam de APIs do sistema e sensores internos, fornecendo estimativas consistentes mas não equivalentes a medições obtidas com equipamentos especializados, como monitores de corrente externos.

5.3 Trabalhos futuros

Como continuidade, recomenda-se expandir os experimentos para outras operações relevantes em computação em borda, incluindo convoluções, modelos completos em TensorFlow Lite e redes neurais voltadas a classificação de eventos. A incorporação de bibliotecas FFT especializadas, inclusive implementações otimizadas para GPU ou Vulkan, permitiria comparar abordagens e identificar eventuais vantagens sobre o delegate genérico. Também é desejável ampliar o conjunto de dispositivos testados, contemplando SoCs com NPUs dedicadas, perfis energéticos distintos e versões recentes do Android para observar como drivers e firmware impactam o desempenho. Finalmente, aplicar o pipeline a um caso real de monitoramento contínuo — em ambiente industrial ou de saúde — ajudará a validar o impacto das otimizações em sessões prolongadas e a correlacionar os ganhos de latência com melhorias concretas em usabilidade e autonomia.
